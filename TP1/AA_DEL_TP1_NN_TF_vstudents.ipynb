{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110b3ea8",
   "metadata": {},
   "source": [
    "<font  style=\"font-size: 3rem; color: darkviolet\"> Neural Networks in TensorFlow </font>\n",
    "\n",
    "AA - DEL - 2023/24 - TP1 - 3h\n",
    "\n",
    "Author: Francesca Galassi\n",
    "\n",
    "*This assignment is inspired by the Deep Learning course on Coursera by Andrew Ng, Stanford University, for which we are thankful.*\n",
    "\n",
    "**Submit this notebook with your solutions, answers and observations.**\n",
    "\n",
    "In this practical, you will learn how to use TensorFlow, a powerful framework that simplifies the creation and training of neural networks. TensorFlow, along with other tools like Torch, can significantly accelerate your progress in deep learning development.\n",
    "\n",
    "You'll explore a dataset, preprocess it, initialize network parameters, and build and train a neural network using TensorFlow. The task involves multi-class classification, where the goal is to classify inputs into one of several possible categories.\n",
    "\n",
    "Resources to understand and work with gradients in TensorFlow:\n",
    "\n",
    "Introduction to Gradients and Automatic Differentiation: \n",
    "https://www.tensorflow.org/guide/autodiff \n",
    "\n",
    "GradientTape documentation:\n",
    "https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "- [1 - Dataset](#1)\n",
    "- [2 - One Hot Encoding](#2)\n",
    "- [3 - Parameters Initialization](#3)\n",
    "- [4 - Building a Neural Network in TensorFlow](#4)\n",
    "    - [4.1 - Forward Propagation](#4.1)\n",
    "    - [4.2 - Total Loss](#4.2)\n",
    "    - [4.3 - Training the Model](#4.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de70dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework.ops import EagerTensor\n",
    "from tensorflow.python.ops.resource_variable_ops import ResourceVariable\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33fcbf9",
   "metadata": {},
   "source": [
    "You will be using v2.x, for maximum speed and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fa9d224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc9235a",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## <font color='darkviolet'> 1 - Dataset\n",
    "\n",
    "The `tf.Tensor` object is a fundamental data structure in TensorFlow used for manipulating and storing numerical data. Tensors are similar to NumPy arrays, as they are multidimensional arrays with specific data types and contain information about the computational graph.\n",
    "    \n",
    "Using TensorFlow datasets created from HDF5 files offers advantages in terms of efficiency and scalability. It is a more efficient alternative to using NumPy arrays for storing large datasets, as it enables you to work with streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ac7ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the HDF5 files\n",
    "train_dataset = h5py.File('data/train_signs.h5', \"r\")\n",
    "test_dataset = h5py.File('data/test_signs.h5', \"r\")\n",
    "\n",
    "# Create datasets from HDF5 datasets\n",
    "x_train = tf.data.Dataset.from_tensor_slices(train_dataset['train_set_x'])\n",
    "y_train = tf.data.Dataset.from_tensor_slices(train_dataset['train_set_y'])\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(test_dataset['test_set_x'])\n",
    "y_test = tf.data.Dataset.from_tensor_slices(test_dataset['test_set_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c899d39",
   "metadata": {},
   "source": [
    "#### <font color='blue'>  Exercise 1. \n",
    "\n",
    "Exploring and describing the dataset is a fundamental initial task in any deep learning project. Use the code and steps provided below to explore and describe the dataset. Use appropriate TensorFlow methods or attributes to determine information about the dataset, including shape, data type, and the number of unique classes or labels. Preprocess the data as indicated.\n",
    "    \n",
    "In TensorFlow, datasets are designed as iterable data sources. You typically iterate over them using a for loop or explicitly create a Python iterator using `iter` and consume its elements using `next`. In addition, you can examine the `shape` and `dtype` of each element in the dataset using the `element_spec` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1431aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 17:20:54.230830: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Examine the shape and data type of the elements using the element_spec attribute.\n",
    "# TODO\n",
    "\n",
    "\n",
    "# Examine labels\n",
    "unique_labels = set()\n",
    "for element in y_train:\n",
    "    unique_labels.add(element.numpy())\n",
    "print(unique_labels)\n",
    "\n",
    "# Iterate over the dataset to display the first 10 images and labels\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc3e97e",
   "metadata": {},
   "source": [
    "Normalization is a crucial preprocessing step that can improve training and convergence of deep neural network. By scaling pixel values to a consistent range, normalization ensures that the model's optimization algorithms operate effectively.\n",
    "\n",
    "The `normalize` function performs this preprocessing on an image tensor. It first converts the data type of the tensor if needed, then normalizes the pixel values to a standard scale. Finally, it flattens the tensor into a 1D representation, which is more convenient for subsequent processing by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c479b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "\n",
    "    # Convert the data type of the input image to float32 and normalize its components\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    \n",
    "    # Reshape the image tensor into a 1D vector\n",
    "    image = tf.reshape(image, [-1,])\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b501a2e8",
   "metadata": {},
   "source": [
    "In TensorFlow datasets, the `map()` method enables you to apply a function to each element within the dataset. This function can be customized to perform any desired operation or transformation on the dataset elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab7be35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply normalization to each image in the dataset\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8837b35",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## <font color='darkviolet'> 2 - One Hot Encoding\n",
    "\n",
    "In deep learning, we often represent labels as vectors ranging from $0$ to $C-1$, where $C$ is the number of classes. To work with these labels effectively, we use \"one-hot encoding\". This technique converts class labels into numerical vectors, making them compatible with machine learning models and facilitating interpretation of model predictions.\n",
    "    \n",
    "Each class is represented by a vector where only one element (the one corresponding to the class) is \"hot\" (set to 1), while all other elements are \"cold\" (set to 0). \n",
    "    \n",
    "For example, for $C=4$, the labels vector might be:\n",
    "\n",
    "<img src=\"data/one_hot.png\" style=\"width:600px;height:150px;\">\n",
    "    \n",
    "In TensorFlow, you can easily perform one-hot encoding using the `tf.one_hot()` function. This function takes the labels and the total number of classes (`depth`) as input and returns a one-hot encoded representation of the labels.\n",
    "    \n",
    "`tf.one_hot(labels, depth, axis=0)`https://www.tensorflow.org/api_docs/python/tf/one_hot\n",
    "   \n",
    "<a name='ex-2'></a>\n",
    "#### <font color='blue'> Exercise 2.\n",
    "    \n",
    "The function `one_hot_matrix` takes a single label and the total number of classes `depth` as input and returns the one-hot encoding as a single-row matrix. It first uses `tf.one_hot()` to generate the one-hot encoding for the label and then reshapes the tensor to have a single row, flattening it using `tf.reshape()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d548a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(label, depth=6):\n",
    "    \"\"\"\n",
    "    Computes the one-hot encoding for a single label.\n",
    "\n",
    "    Arguments:\n",
    "        label -- (int) Categorical label.\n",
    "        depth -- (int) Number of different classes that label can take.\n",
    "\n",
    "    Returns:\n",
    "         one_hot -- tf.Tensor A single-row matrix with the one-hot encoding.\n",
    "    \"\"\"\n",
    "    # Use the built-in TensorFlow function tf.one_hot to compute the one-hot encoding for the label.\n",
    "    # TODO\n",
    "\n",
    "    \n",
    "    # Reshape the tensor to have a single row and flatten it.\n",
    "    # TODO\n",
    "\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eaaadbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'one_hot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(result, [\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m ,\u001b[38;5;241m1.\u001b[39m, \u001b[38;5;241m0.\u001b[39m] ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong output. Use tf.reshape as instructed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[92mAll test passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mone_hot_matrix_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_hot_matrix\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m, in \u001b[0;36mone_hot_matrix_test\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m      2\u001b[0m label \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m----> 4\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest 1:\u001b[39m\u001b[38;5;124m\"\u001b[39m,result)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m result\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m depth, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse the parameter depth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m, in \u001b[0;36mone_hot_matrix\u001b[0;34m(label, depth)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mComputes the one-hot encoding for a single label.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m     one_hot -- tf.Tensor A single-row matrix with the one-hot encoding.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Use the built-in TensorFlow function tf.one_hot to compute the one-hot encoding for the label.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Reshape the tensor to have a single row and flatten it.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mone_hot\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'one_hot' is not defined"
     ]
    }
   ],
   "source": [
    "def one_hot_matrix_test(target):\n",
    "    label = tf.constant(1)\n",
    "    depth = 4\n",
    "    result = target(label, depth)\n",
    "    print(\"Test 1:\",result)\n",
    "    assert result.shape[0] == depth, \"Use the parameter depth\"\n",
    "    assert np.allclose(result, [0., 1. ,0., 0.] ), \"Wrong output. Use tf.one_hot\"\n",
    "    label_2 = [2]\n",
    "    result = target(label_2, depth)\n",
    "    print(\"Test 2:\", result)\n",
    "    assert result.shape[0] == depth, \"Use the parameter depth\"\n",
    "    assert np.allclose(result, [0., 0. ,1., 0.] ), \"Wrong output. Use tf.reshape as instructed\"\n",
    "    \n",
    "    print(\"\\033[92mAll test passed\")\n",
    "\n",
    "one_hot_matrix_test(one_hot_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc20fc56",
   "metadata": {},
   "source": [
    "Use the `map()` method in TensorFlow datasets to apply the `one_hot_matrix()` function to each element in `y_test` and `y_train`, thereby creating new datasets `new_y_test` and `new_y_train` containing the one-hot encoded labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abdc3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d177b",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## <font color='darkviolet'> 3 - Parameters Initialization\n",
    "  \n",
    "Using an appropriate initializer, like the Glorot initializer, helps ensure that the neural network starts training from a good initialization point. This leads to more effective and efficient learning, as the optimization algorithm can converge faster and more reliably to a good solution.\n",
    "    \n",
    "In this step, you will initialize the neural network model parameters using the Glorot initializer, which can be accessed through `tf.keras.initializers.GlorotNormal`.\n",
    "It creates values from a normal distribution centered at 0, with a standard deviation of `sqrt(2 / (fan_in + fan_out))`. Here, `fan_in` represents the number of input units, and `fan_out` represents the number of output units in the weight tensor.\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "#### <font color='blue'> Exercise 3.\n",
    "\n",
    "Complete the function using:\n",
    "\n",
    " - `tf.keras.initializers.GlorotNormal(seed=1)`\n",
    " - `tf.Variable(initializer(shape=())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e534b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with TensorFlow. The shapes are given:\n",
    "    \n",
    "                        W1 : [25, 12288]  --> [hidden layer size, input size]\n",
    "                        b1 : [25, 1]\n",
    "                        W2 : [12, 25]\n",
    "                        b2 : [12, 1]\n",
    "                        W3 : [6, 12]\n",
    "                        b3 : [6, 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "                                \n",
    "    initializer = tf.keras.initializers.GlorotNormal(seed=1)\n",
    "    \n",
    "    # Initialize parameters for each layer of the neural network\n",
    "    # TODO\n",
    "\n",
    "\n",
    "    # Dictionary\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf0327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_test(target):\n",
    "    parameters = target()\n",
    "\n",
    "    values = {\"W1\": (25, 12288),\n",
    "              \"b1\": (25, 1),\n",
    "              \"W2\": (12, 25),\n",
    "              \"b2\": (12, 1),\n",
    "              \"W3\": (6, 12),\n",
    "              \"b3\": (6, 1)}\n",
    "\n",
    "    for key in parameters:\n",
    "        print(f\"{key} shape: {tuple(parameters[key].shape)}\")\n",
    "        assert type(parameters[key]) == ResourceVariable, \"All parameter must be created using tf.Variable\"\n",
    "        assert tuple(parameters[key].shape) == values[key], f\"{key}: wrong shape\"\n",
    "        assert np.abs(np.mean(parameters[key].numpy())) < 0.5,  f\"{key}: Use the GlorotNormal initializer\"\n",
    "        assert np.std(parameters[key].numpy()) > 0 and np.std(parameters[key].numpy()) < 1, f\"{key}: Use the GlorotNormal initializer\"\n",
    "\n",
    "    print(\"\\033[92mAll test passed\")\n",
    "\n",
    "initialize_parameters_test(initialize_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bda53e",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## <font color='darkviolet'> 4 - Building a Neural Network in TensorFlow\n",
    "\n",
    "In this section, you will build a neural network. This process involves two major steps: i) implementing forward propagation and ii) retrieving the gradients.\n",
    "\n",
    "<a name='4.1'></a>\n",
    "### <font color='darkviolet'> 4.1 - Forward Propagation \n",
    "\n",
    "TensorFlow simplifies neural network construction by automatically tracking operations for backpropagation. You only need to implement the forward propagation function, and it will keep track of the operations you did to calculate the back propagation automatically.  \n",
    "\n",
    "<a name='ex-4'></a>\n",
    "#### <font color='blue'> Exercise 4.1.\n",
    "\n",
    "Complete the `forward_propagation` function using:\n",
    "- `tf.math.add`\n",
    "- `tf.linalg.matmul`\n",
    "- `tf.keras.activations.relu`\n",
    "    \n",
    "Then, explain the role of the ReLU activation function in a neural network.\n",
    "\n",
    "Bonus: What Numpy functions are equivalent to the operations performed in the `TODO` lines of the `forward_propagation` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR\n",
    "    Args:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- dictionary containing the parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "\n",
    "    # Implement forward propagation\n",
    "    # TODO\n",
    "\n",
    "\n",
    "    return Z3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_test(target, examples):\n",
    "    minibatches = examples.batch(2)\n",
    "    parametersk = initialize_parameters()\n",
    "    W1 = parametersk['W1']\n",
    "    b1 = parametersk['b1']\n",
    "    W2 = parametersk['W2']\n",
    "    b2 = parametersk['b2']\n",
    "    W3 = parametersk['W3']\n",
    "    b3 = parametersk['b3']\n",
    "    index = 0\n",
    "    minibatch = list(minibatches)[0]\n",
    "    with tf.GradientTape() as tape:\n",
    "        forward_pass = target(tf.transpose(minibatch), parametersk)\n",
    "        print(forward_pass)\n",
    "        fake_cost = tf.reduce_mean(forward_pass - np.ones((6,2)))\n",
    "\n",
    "        assert type(forward_pass) == EagerTensor, \"Your output is not a tensor\"\n",
    "        assert forward_pass.shape == (6, 2), \"Last layer must use W3 and b3\"\n",
    "        assert np.allclose(forward_pass, \n",
    "                           [[-0.13430887,  0.14086473],\n",
    "                            [ 0.21588647, -0.02582335],\n",
    "                            [ 0.7059658,   0.6484556 ],\n",
    "                            [-1.1260961,  -0.9329492 ],\n",
    "                            [-0.20181894, -0.3382722 ],\n",
    "                            [ 0.9558965,   0.94167566]]), \"Output does not match\"\n",
    "    index = index + 1\n",
    "    trainable_variables = [W1, b1, W2, b2, W3, b3]\n",
    "    grads = tape.gradient(fake_cost, trainable_variables)\n",
    "    assert not(None in grads), \"Wrong gradients. It could be due to the use of tf.Variable whithin forward_propagation\"\n",
    "    print(\"\\033[92mAll test passed\")\n",
    "\n",
    "forward_propagation_test(forward_propagation, new_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780fa06a",
   "metadata": {},
   "source": [
    "<a name='4.2'></a>\n",
    "### <font color='darkviolet'> 4.2 - Total Loss\n",
    "\n",
    "In this multi-class classification problem, you will use categorical cross-entropy as the loss function. It measures the dissimilarity between predicted class probabilities and true labels, with predicted probabilities generated using the softmax function. This function takes a vector of arbitrary real-valued scores (often called logits) as input and converts them into a probability distribution across multiple classes. The softmax output is a vector where each element denotes the probability of the corresponding class.\n",
    "   \n",
    "During training, the total loss will be used to adjust the model's parameters via backpropagation. Indeed, when handling mini-batches of data during training, opting for the total loss over the average loss ensures consistency in assessing the model's performance. For example, if you have 5 samples but process 4 at a time, the last mini-batch will have only 1 sample. By using the total loss, you ensure that each sample contributes equally to the overall evaluation, leading to consistent training outcomes.\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "#### <font color='blue'> Exercise 4.2.\n",
    "    \n",
    "Implement the total loss function using:\n",
    "    \n",
    "- `tf.nn.softmax_cross_entropy_with_logits`\n",
    "\n",
    "- `tf.reduce_sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c9192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_loss(logits, labels):\n",
    "    \"\"\"\n",
    "    Computes the total loss.\n",
    "\n",
    "    Arguments:\n",
    "    logits -- the output of forward propagation (the output of the last LINEAR unit), of shape (num_examples, num_classes)\n",
    "    labels -- the \"true\" labels vector, of the same shape as logits\n",
    "\n",
    "    Returns:\n",
    "    total_loss -- a Tensor representing the total loss value   \n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the individual losses.    \n",
    "    # TODO -  axis: the class dimension. \n",
    "\n",
    "    \n",
    "    # Compute the total loss as the sum of the individual losses.\n",
    "    # TODO\n",
    "\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c7088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_loss_test(target, Y):\n",
    "    # Tensor with predicted values. \n",
    "    # Note: the shape of pred is (num_classes, num_examples). 6 classes and 2 examples.\n",
    "    pred = tf.constant([[ 2.4048107,   5.0334096 ],\n",
    "             [-0.7921977,  -4.1523376 ],\n",
    "             [ 0.9447198,  -0.46802214],\n",
    "             [ 1.158121,    3.9810789 ],\n",
    "             [ 4.768706,    2.3220146 ],\n",
    "             [ 6.1481323,   3.909829  ]])\n",
    "    \n",
    "    #print(f\"pred shape: {pred.shape}\")\n",
    "    \n",
    "    # A batch size of 2\n",
    "    minibatches = Y.batch(2)\n",
    "    for minibatch in minibatches:\n",
    "        \n",
    "        print(f\"Minibatch shape: {tf.transpose(minibatch)}\")\n",
    "        # Note: transposes the minibatch to match the shape of the predicted values.\n",
    "        result = target(pred, tf.transpose(minibatch))\n",
    "        break\n",
    "        \n",
    "    assert(type(result) == EagerTensor), \"Use the TensorFlow API\"\n",
    "    assert (np.abs(result - (0.50722074 + 1.1133534) / 2.0) < 1e-7), \"Test does not match. Did you get the reduce sum of your loss functions?\"\n",
    "\n",
    "    print(\"\\033[92mAll test passed\")\n",
    "\n",
    "print(next(iter(new_y_train)).shape)\n",
    "compute_total_loss_test(compute_total_loss, new_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b396aec5",
   "metadata": {},
   "source": [
    "<a name='4.3'></a>\n",
    "### <font color='darkviolet'> 4.3 - Training the Model\n",
    "\n",
    "When training a model, you need to use an optimizer to update the parameters and improve its accuracy. You can specify the optimizer, such as `tf.keras.optimizers.Adam`, in just one line of code. There are other optimizers available, like Stochastic Gradient Descent.\n",
    "\n",
    "Next, you'll use `tape.gradient` to calculate the gradients of the loss function with respect to the model's parameters. Then, you'll use the optimizer's `apply_gradients` method to update the model's parameters using the calculated gradients.\n",
    "\n",
    "An additional step has been added to the batch training process: `dataset = dataset.prefetch(8)`. This step helps prevent a memory bottleneck that can occur when reading data from disk. By prefetching the data, it's kept ready for when it's needed and processed in smaller, more manageable pieces to improve efficiency.\n",
    "    \n",
    "<a name='ex-6'></a>\n",
    "#### <font color='blue'> Exercise 4.3.\n",
    "    \n",
    "Complete the code as indicated. Then, observe and describe the learning curves on both training and test datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff46c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate=0.0001,\n",
    "          num_epochs=1500, minibatch_size=32, print_cost=True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer TensorFlow neural network: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (number of training examples, input size)\n",
    "    Y_train -- training labels, of shape (number of training examples, output size)\n",
    "    X_test -- test set, of shape (number of test examples, input size)\n",
    "    Y_test -- test labels, of shape (number of test examples, output size)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 10 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model\n",
    "    costs -- list of costs over the training epochs\n",
    "    train_acc -- list of training accuracies over the training epochs\n",
    "    test_acc -- list of test accuracies over the training epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize cost and accuracy lists\n",
    "    costs = []                                      \n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    \n",
    "    # Initialize parameters\n",
    "    # TODO\n",
    "\n",
    "    \n",
    "    # Unpack the initialized parameters into individual variables\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "\n",
    "    # Adam optimizer\n",
    "    # TODO\n",
    "\n",
    "    \n",
    "    # Create accuracy metrics\n",
    "    train_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    test_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    \n",
    "    # Create TensorFlow datasets for the training and test sets\n",
    "    dataset = tf.data.Dataset.zip((X_train, Y_train))\n",
    "    test_dataset = tf.data.Dataset.zip((X_test, Y_test))\n",
    "       \n",
    "    # Get the number of training examples\n",
    "    m = dataset.cardinality().numpy()\n",
    "    \n",
    "    # Create minibatches of the training and test datasets\n",
    "    minibatches = dataset.batch(minibatch_size).prefetch(8)\n",
    "    test_minibatches = test_dataset.batch(minibatch_size).prefetch(8)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        epoch_total_loss = 0.\n",
    "        \n",
    "        # Reset accuracy at each epoch\n",
    "        train_accuracy.reset_states()\n",
    "\n",
    "        # Iterate through each minibatch in the training set\n",
    "        for minibatch_X, minibatch_Y in minibatches:\n",
    "            \n",
    "            # Print the shape of minibatch_X and minibatch_Y\n",
    "            #print(f\"Minibatch_X shape: {minibatch_X.shape}\")\n",
    "            #print(f\"Minibatch_Y shape: {minibatch_Y.shape}\")\n",
    "            \n",
    "            # Use a gradient tape to record gradients for backpropagation\n",
    "            with tf.GradientTape() as tape:\n",
    "                \n",
    "                # Perform forward propagation\n",
    "                # TODO\n",
    "\n",
    "                \n",
    "                # Compute the loss\n",
    "                # TODO\n",
    "                #print(f\"Minibatch_Z3 shape: {Z3.shape}\")\n",
    "\n",
    "                \n",
    "            # Update accuracy metric for this minibatch\n",
    "            train_accuracy.update_state(minibatch_Y, tf.transpose(Z3))\n",
    "\n",
    "            # Compute gradients and update parameters using the optimizer\n",
    "            trainable_variables = [W1, b1, W2, b2, W3, b3]\n",
    "            grads = tape.gradient(minibatch_total_loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "\n",
    "            # Add the minibatch loss to the epoch total\n",
    "            epoch_total_loss += minibatch_total_loss\n",
    "\n",
    "        # Compute the average total loss of the current epoch\n",
    "        epoch_total_loss /= m\n",
    "\n",
    "        # Print the cost every 10 epochs\n",
    "        if print_cost and epoch % 10 == 0:\n",
    "            print(f\"Cost after epoch {epoch}: {epoch_total_loss:.4f}\")\n",
    "            print(f\"Train accuracy: {train_accuracy.result():.4f}\")\n",
    "\n",
    "            # Compute accuracy on the test set\n",
    "            # TODO\n",
    "\n",
    "            \n",
    "            # Record cost and accuracy metrics\n",
    "            # TODO\n",
    "\n",
    "            \n",
    "    return parameters, costs, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ac0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs, train_acc, test_acc = model(new_train, new_y_train, new_test, new_y_test, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67121348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cost\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dced30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the train and test accuracy\n",
    "# TODO\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
