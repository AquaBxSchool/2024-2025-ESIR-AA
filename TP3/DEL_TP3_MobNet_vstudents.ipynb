{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110b3ea8",
   "metadata": {},
   "source": [
    "<font style=\"font-size: 3rem; color: darkviolet\"> MobileNets and Transfer Learning </font> \n",
    "\n",
    "AA - DEL - 2023/24 - TP3 - 3h\n",
    "\n",
    "Author: Francesca Galassi\n",
    "\n",
    "This assignment is inspired by the Deep Learning course on Coursera by Andrew Ng, Stanford University, for which we are thankful.\n",
    "\n",
    "**Submit this notebook with your solutions, answers and observations.**\n",
    "\n",
    "In this assignment, your task is to employ transfer learning on a pre-trained Convolutional Neural Network, MobileNetV2, to construct an Alpaca/Not Alpaca classifier. \n",
    "\n",
    "MobileNetV2 has already undergone training on the large ImageNet dataset, containing over 14 million images and 1000 classes. \n",
    "\n",
    "Your task is to adapt MobileNetV2 to your specific classification problem: distinguishing between alpacas and other images.\n",
    "\n",
    "#### Main objectives:\n",
    "\n",
    "- Augment the training dataset to increase its diversity and improve model generalization.\n",
    "- Adapt the pre-trained MobileNetV2 model to the new dataset and classification task of distinguishing between alpacas and other images.\n",
    "- Fine-tune the layers of the classifier to further improve the accuracy of the model.\n",
    "\n",
    "Reminder: \n",
    "\n",
    "- *Epoch*: One complete pass through the entire training dataset. During an epoch, the model learns from all training examples exactly once.\n",
    "\n",
    "- *Batch*: A subset of the training dataset processed in one forward pass and one backward pass. Weights are updated after processing each batch.\n",
    "\n",
    "- *Iterations*: The process of updating the weights based on the gradients computed from one batch of data. One iteration is completed when a batch of data has been processed.\n",
    "\n",
    "- *Training Steps*: The total number of steps to complete one epoch, equal to the total number of training examples divided by the batch size.\n",
    "\n",
    "- *Optimizer*: An algorithm used to minimize the loss function by updating the weights of the neural network during training. Common optimizers include Stochastic Gradient Descent (SGD), Adam, RMSprop, and Adagrad. Documentation: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "\n",
    "### Table of Contents\n",
    "- [1 - Dataset Preparation and Data Augmentation](#1)\n",
    "- [2 - Using MobileNetV2 for Transfer Learning](#2)\n",
    "    - [2.1 - Training the top layers](#2.1)\n",
    "    - [2.2 - Fine-tuning the model](#2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de70dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe16eac",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## <font color='darkviolet'> 1 - Dataset Preparation and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427dd393",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "### <font color='blue'> Exercise 1 \n",
    "    \n",
    "(i.) Data augmentation involves applying transformations to existing training samples to increase dataset diversity, thereby improving model generalization and performance. You apply augmentation techniques such as rotation, shifting, and flipping to the training set using `tf.keras.preprocessing.image.ImageDataGenerator`. This generator creates augmented images on-the-fly during training. \n",
    "    \n",
    "To use `tf.keras.preprocessing.image.ImageDataGenerator`:\n",
    "1. For the training dataset, initialize the `ImageDataGenerator` with augmentation parameters, such as rotation* and flipping**, while for the validation dataset, initialize it only for loading images and rescaling them without any augmentation. \n",
    "2. Use the `flow_from_directory()` method to generate augmented images from the directory containing the dataset. This method creates an iterator that yields batches of images and their corresponding labels. Specify the path to the dataset directory, target image size, batch size, class mode, etc.\n",
    "3. In Exercise 2, after defining and compiling a model, use the `fit()` method to train the model. Pass the `train_generator` and `val_generator` objects as arguments. This way, the generators handle the data loading and augmentation on-the-fly during training.\n",
    "    \n",
    "Consult the official documentation for `tf.keras.preprocessing.image.ImageDataGenerator` for guidance on its usage and available parameters:\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator    \n",
    "    \n",
    "\n",
    "*Note*: Ensuring the use of the same `seed` guarantees consistency in the split between training and validation datasets. Also, setting `rescale=1./255` normalizes pixel values to the range [0, 1].\n",
    "   \n",
    "*Rotation: `rotation_range=0.2` randomly rotate images by a degree range.\n",
    "    \n",
    "**Flipping: `horizontal_flip=True` randomly flips images horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ad770d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ImageDataGenerator.flow_from_directory() missing 1 required positional argument: 'directory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m validation_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m \u001b[38;5;66;03m# validation split (20%)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m train_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_datagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m val_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m val_generator \u001b[38;5;241m=\u001b[39m val_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m     20\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: ImageDataGenerator.flow_from_directory() missing 1 required positional argument: 'directory'"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/\"\n",
    "batch_size = 32 # batch size for training\n",
    "img_size = (160, 160) # target image size for resizing\n",
    "validation_split = 0.2 # validation split (20%)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    # TODO\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    # TODO\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    # TODO\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    # TODO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc1e46",
   "metadata": {},
   "source": [
    "(ii.) Explore the dimensions, size, labels, and plot a few images from the training dataset. The images are accessed directly from the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ba694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d177b",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "### <font color='darkviolet'>2 - Using MobileNetV2 for Transfer Learning\n",
    "\n",
    "In transfer learning, we leverage the knowledge learned by a model on a large dataset and apply it to a new, possibly smaller dataset. \n",
    "    \n",
    "MobileNetV2 (refer to the last lecture) was trained on the ImageNet dataset for the task of image classification. ImageNet is a widely used dataset for image classification tasks, containing millions of images across thousands of categories. \n",
    "This pre-trained model serves as a starting point for transfer learning on the similar task of binary image classification on the Alpaca dataset.\n",
    "\n",
    "*Note*: Since MobileNetV2 was trained with normalization values in the range of [-1, 1], it's best to normalize your input data similarly. You may achieve this easily by using the `tf.keras.applications.mobilenet_v2.preprocess_input` function along with the data generator from the previous exercise.\n",
    "    \n",
    "Let's obtain the pre-trained MobileNetV2 model with weights learned from the ImageNet dataset and examine its architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MobileNetV2 \n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(160, 160, 3), include_top=True, weights='imagenet')\n",
    "\n",
    "# Observe the architecture\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbfd0f1",
   "metadata": {},
   "source": [
    "*Note*: The include_top=True argument includes the fully connected layers (top layers) of the MobileNetV2 model, which are designed for the original classification task on ImageNet's 1000 classes. However, for the binary classification task on the Alpaca dataset, you will need to remove these top layers and add your own classification layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00759bc8",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "### <font color='blue'> Exercise 2.1 - Training the top layers only\n",
    "    \n",
    "(i.) Adapting the pre-trained model for the targeted task of recognizing alpacas involves the following steps:\n",
    "\n",
    "1. Remove the top layers used for the original classification task.\n",
    "2. Add a new classification layer specifically designed for the task of recognizing alpacas. This new layer will have the appropriate number of output units for binary classification.\n",
    "3. Freeze the base model to ensure that the existing weights remain unchanged. This allows only training on the newly introduced layers. The frozen layers act as a feature extractor, and the training process focuses on the top layers for the specific task.\n",
    "    \n",
    "Consult the documentation : https://keras.io/guides/transfer_learning/#introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf67fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load MobileNetV2 \n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8084858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a new model with the MobileNetV2 base and a new classification layer\n",
    "model_alpaca = tf.keras.models.Sequential([\n",
    "    # TODO\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b464ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Freeze the base model\n",
    "# TODO\n",
    "\n",
    "# Compile the model\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3755cdba",
   "metadata": {},
   "source": [
    "(ii.) Train the model. Plot and observe the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b71afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b396aec5",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### <font color='blue'> Exercise 2.2 - Fine-tuning the model\n",
    "\n",
    "In fine-tuning, we adjust the parameters of the pre-trained model to better fit the specifics of the new dataset and task. Since the pre-trained weights are already quite good, we usually use a small learning rate to make smaller adjustments to the weights during training.\n",
    "\n",
    "Typically, the early layers of the model capture more generic features (like edges and textures) that are useful across different tasks, while later layers are more specific to the task at hand, such as distinguishing alpacas based on distinctive features like pointy ears and hairy tails.\n",
    "\n",
    "(i.) To initiate the fine-tuning process, unfreeze the layers at the end of the network. Specify the layer from which fine-tuning begins and re-freeze all preceding layers. Re-run the training for additional epochs and assess whether this fine-tuning improves the model's accuracy. Feel free to experiment with the starting layer for fine-tuning, as the specific choice is somewhat arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dd86bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848c25c9",
   "metadata": {},
   "source": [
    "(ii.) Fine-tune the model for additional epochs. Plot and observe the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c275fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with a lower learning rate for fine-tuning\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c482a43c",
   "metadata": {},
   "source": [
    "**Experiment with different strategies for data augmentation, fine-tuning, and custom layers.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
