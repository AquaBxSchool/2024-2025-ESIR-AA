{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qo4gVJ_D01SH"
   },
   "source": [
    "<font style=\"font-size: 3rem; color: darkviolet\"> Character level language model - Dinosaurus Island </font> \n",
    "\n",
    "AA - DEL - 2023/24 - TP4 - 3h\n",
    "\n",
    "Author: Francesca Galassi\n",
    "\n",
    "*This assignment is inspired by the Deep Learning course on Coursera by Andrew Ng, Stanford University, for which we are thankful.*\n",
    "\n",
    "**Submit this notebook with your solutions, answers and observations.**\n",
    "\n",
    "Welcome to Dinosaurus Island! Dinosaurs existed 65 million years ago, and now they're back.\n",
    "\n",
    "Researchers are creating new breeds of dinosaurs and bringing them to life on Earth. Your task is to give names to these creatures. But be careful - if a dinosaur doesn't like its name, it might get angry!\n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/dino.jpg\" style=\"width:150;height:200px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>\n",
    "\n",
    "Luckily, you're equipped with some deep learning. Your assistant has collected a list of all the dinosaur names they could find and compiled them into this [dataset](dinos.txt). To create new dinosaur names, you will build a character-level language model to generate new names. Your algorithm will learn different name patterns and randomly generate new names. Hopefully, this algorithm will keep you and your team safe from the dinosaurs' anger!\n",
    "\n",
    "By completing this assignment, you'll be able to:\n",
    "\n",
    "* Prepare text data for processing with an RNN \n",
    "* Sample novel sequences with an RNN\n",
    "* Understand and fix issues with gradients in RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1 - Problem Statement](#1)\n",
    "    - [1.1 - Dataset and Preprocessing](#1-1)\n",
    "    - [1.2 - Overview of the Model](#1-2)\n",
    "- [2 - Building Blocks of the Model](#2)\n",
    "    - [2.1 - Clipping the Gradients in the Optimization Loop](#2-1)\n",
    "        - [Exercise 1](#ex-1)\n",
    "    - [2.2 - Sampling](#2-2)\n",
    "        - [Exercise 2](#ex-2)\n",
    "- [3 - The Language Model](#3)\n",
    "    - [3.1 - Gradient Descent](#3-1)\n",
    "        - [Exercise 3](#ex-3)\n",
    "    - [3.2 - Training the Model](#3-2)\n",
    "        - [Exercise 4](#ex-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r0Nj4psY01SJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "import random\n",
    "import pprint\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_elAxqq01SN"
   },
   "source": [
    "<a name='1'></a>\n",
    "## <font color='darkviolet'> 1 - Problem Statement\n",
    "\n",
    "<a name='1-1'></a>\n",
    "### <font color='darkviolet'> 1.1 - Dataset and Preprocessing\n",
    "\n",
    "Run the following code to read the dataset of dinosaur names, convert it to lowercase, and create a list of unique characters (such as a-z). Then, we'll compute the dataset size and the size of our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qB2XWVg_01SO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data= data.lower() # Convert to lowercase\n",
    "chars = list(set(data)) # Create a list of unique characters\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.'  % (data_size, vocab_size)) # Compute dataset and vocabulary size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CfmL668r01SQ"
   },
   "source": [
    "The characters include a-z (26 characters) plus the \"\\n\" (newline character). The newline character \"\\n\" indicates the end of a dinosaur name, similar to the <EOS> token discussed in the lecture.\n",
    "    \n",
    "We'll create two dictionaries for the RNN to handle characters: \n",
    "* `char_to_ix`: this dictionary, i.e., a hash table, maps each character to an index from 0-26.\n",
    "* `ix_to_char`: this dictionary maps each index back to the corresponding character. \n",
    "    -  This will help you figure out which index corresponds to which character in the probability distribution output of the softmax layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bh3QcYpr01SQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(chars) # Sort the characters\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2YltsxeZ01SU"
   },
   "outputs": [],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) } # Map characters to indices\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) } # Map indices to characters\n",
    "#pp = pprint.PrettyPrinter(indent=4)\n",
    "#pp.pprint(char_to_ix)\n",
    "#pp.pprint(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OuDQfApB01SW"
   },
   "source": [
    "<a name='1-2'></a>\n",
    "### <font color='darkviolet'> 1.2 - Overview of the Model<a name='1-2'></a>\n",
    "\n",
    "Your model will have the following structure: \n",
    "\n",
    "- Initialize parameters: set up the initial weights and biases.\n",
    "- Run the optimization loop:\n",
    "    - Forward propagation to compute the loss function.\n",
    "    - Backward propagation to compute the gradients with respect to the loss function.\n",
    "    - Clip the gradients to prevent exploding gradients.\n",
    "    - Update parameters using the gradient descent update rule.\n",
    "- Return the learned parameters. \n",
    "    \n",
    "\n",
    "During training, at each time-step, the Recurrent Neural Network (RNN) tries to predict the next character based on the previous characters it has seen in the sequence.\n",
    "    \n",
    "<img src=\"images/rnn.png\" style=\"width:450;height:300px;\">\n",
    "<caption><center><font color='purple'><b>Figure 1</b>: Recurrent Neural Network  </center></caption>\n",
    "    \n",
    "If we represent the input sequence as $\\mathbf{X} = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$, where each $x^{\\langle t \\rangle}$ is a character from the training set, the output sequence $\\mathbf{Y} = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$ is the same list of characters but shifted one character forward.\n",
    "At every time-step $t$, $y^{\\langle t \\rangle} = x^{\\langle t+1 \\rangle}$, meaning that the prediction at time $t$ is the same as the input at time $t + 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tuVLtQaM01SX"
   },
   "source": [
    "<a name='2'></a>\n",
    "## <font color='darkviolet'> 2 - Building Blocks of the Model\n",
    "\n",
    "<a name='2-1'></a>\n",
    "### <font color='darkviolet'> 2.1 - Clipping the Gradients in the Optimization Loop\n",
    "    \n",
    "In this section, you will implement the `clip` function to prevent exploding gradients during training.\n",
    "\n",
    "#### Exploding gradients\n",
    "* When gradients become large, they're called \"exploding gradients.\"  \n",
    "* Exploding gradients make the training process more difficult, because the updates may be so large that they \"overshoot\" the optimal values during back propagation.\n",
    "\n",
    "Recall that the overall training loop structure consists of:\n",
    "* forward pass, \n",
    "* cost computation, \n",
    "* backward pass, \n",
    "* parameter update. \n",
    "\n",
    "Before updating the parameters, you'll perform gradient clipping to ensure that your gradients don't \"explode\".\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### <font color='blue'> Exercise 1\n",
    "    \n",
    "In this exercise, you will implement a function `clip` that takes a dictionary of gradients and returns a clipped version if needed.\n",
    "\n",
    "* There are different ways to clip gradients.You will use a simple element-wise clipping procedure, where each element of the gradient vector is clipped to a range of [-N, N]. \n",
    "* For example, if N=10:\n",
    "    - The range is [-10, 10].\n",
    "    - If any gradient component is greater than 10, it is set to 10.\n",
    "    - If any gradient component is less than -10, it is set to -10. \n",
    "    - Components between -10 and 10 keep their original values.\n",
    "\n",
    "Visualization of gradient descent with and without gradient clipping, in a case where the network is running into \"exploding gradient\" problems:\n",
    "    \n",
    "<img src=\"images/clip.png\" style=\"width:350;height:125px;\">\n",
    "<caption><center><font color='purple'><b>Figure 2</b>: The \"exploding gradient\" problem. </center></caption>\n",
    "   \n",
    "Complete the implementation of the function `clip`. The function `clip` takes in a maximum threshold and returns the clipped versions of the gradients. \n",
    "You will use [numpy.clip](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.clip.html) function for this purpose.\n",
    "    \n",
    "- Note: It's important to use the `out` parameter to update the gradients in-place. If you don't use `out`, the clipped variable is stored in a separate variable, not updating the gradient variables `dWax`, `dWaa`, `dWya`, `db`, `dby`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2yYvYeI501SX"
   },
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "    \n",
    "    Arguments:\n",
    "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "    maxValue -- the threshold value; gradients above this will be set to this value, and gradients below -maxValue will be set to -maxValue\n",
    "    \n",
    "    Returns: \n",
    "    gradients -- a dictionary with the clipped gradients.\n",
    "    '''\n",
    "    gradients = copy.deepcopy(gradients)\n",
    "    \n",
    "    # Extract individual gradients\n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    # Clip gradients to mitigate exploding gradients\n",
    "    for gradient in None:\n",
    "        np.clip(None, None, None, out=None)\n",
    "    \n",
    "    # Update the gradients dictionary\n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NAdY1Eon01Sa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradients for mValue=10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(valuei[index_not_clipped] \u001b[38;5;241m==\u001b[39m valuef[index_not_clipped]), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Problem with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrad\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Some values that should not have changed, changed during the clipping process.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[92mAll tests passed!\u001b[39m\u001b[38;5;130;01m\\x1b\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mclip_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m clip_test(clip, \u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mclip_test\u001b[0;34m(target, mValue)\u001b[0m\n\u001b[1;32m      9\u001b[0m dby \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     10\u001b[0m gradients \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdWax\u001b[39m\u001b[38;5;124m\"\u001b[39m: dWax, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdWaa\u001b[39m\u001b[38;5;124m\"\u001b[39m: dWaa, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdWya\u001b[39m\u001b[38;5;124m\"\u001b[39m: dWya, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m\"\u001b[39m: db, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdby\u001b[39m\u001b[38;5;124m\"\u001b[39m: dby}\n\u001b[0;32m---> 12\u001b[0m gradients2 \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmValue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradients[\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mdWaa\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m][1][2] =\u001b[39m\u001b[38;5;124m\"\u001b[39m, gradients2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdWaa\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradients[\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mdWax\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m][3][1] =\u001b[39m\u001b[38;5;124m\"\u001b[39m, gradients2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdWax\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(gradients, maxValue)\u001b[0m\n\u001b[1;32m     15\u001b[0m dWaa, dWax, dWya, db, dby \u001b[38;5;241m=\u001b[39m gradients[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdWaa\u001b[39m\u001b[38;5;124m'\u001b[39m], gradients[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdWax\u001b[39m\u001b[38;5;124m'\u001b[39m], gradients[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdWya\u001b[39m\u001b[38;5;124m'\u001b[39m], gradients[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m'\u001b[39m], gradients[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdby\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Clip gradients to mitigate exploding gradients\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Update the gradients dictionary\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "def clip_test(target, mValue):\n",
    "    print(f\"\\nGradients for mValue={mValue}\")\n",
    "    np.random.seed(3)\n",
    "    dWax = np.random.randn(5, 3) * 10\n",
    "    dWaa = np.random.randn(5, 5) * 10\n",
    "    dWya = np.random.randn(2, 5) * 10\n",
    "    db = np.random.randn(5, 1) * 10\n",
    "    dby = np.random.randn(2, 1) * 10\n",
    "    gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "\n",
    "    gradients2 = target(gradients, mValue)\n",
    "    print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients2[\"dWaa\"][1][2])\n",
    "    print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients2[\"dWax\"][3][1])\n",
    "    print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients2[\"dWya\"][1][2])\n",
    "    print(\"gradients[\\\"db\\\"][4] =\", gradients2[\"db\"][4])\n",
    "    print(\"gradients[\\\"dby\\\"][1] =\", gradients2[\"dby\"][1])\n",
    "    \n",
    "    for grad in gradients2.keys():\n",
    "        valuei = gradients[grad]\n",
    "        valuef = gradients2[grad]\n",
    "        mink = np.min(valuef)\n",
    "        maxk = np.max(valuef)\n",
    "        assert mink >= -abs(mValue), f\"Problem with {grad}. Set a_min to -mValue in the np.clip call\"\n",
    "        assert maxk <= abs(mValue), f\"Problem with {grad}.Set a_max to mValue in the np.clip call\"\n",
    "        index_not_clipped = np.logical_and(valuei <= mValue, valuei >= -mValue)\n",
    "        assert np.all(valuei[index_not_clipped] == valuef[index_not_clipped]), f\" Problem with {grad}. Some values that should not have changed, changed during the clipping process.\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed!\\x1b[0m\")\n",
    "    \n",
    "clip_test(clip, 10)\n",
    "clip_test(clip, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6O4BLjJf01Sd"
   },
   "source": [
    "**Expected values**\n",
    "```\n",
    "Gradients for mValue=10\n",
    "gradients[\"dWaa\"][1][2] = 10.0\n",
    "gradients[\"dWax\"][3][1] = -10.0\n",
    "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
    "gradients[\"db\"][4] = [10.]\n",
    "gradients[\"dby\"][1] = [8.45833407]\n",
    "\n",
    "Gradients for mValue=5\n",
    "gradients[\"dWaa\"][1][2] = 5.0\n",
    "gradients[\"dWax\"][3][1] = -5.0\n",
    "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
    "gradients[\"db\"][4] = [5.]\n",
    "gradients[\"dby\"][1] = [5.]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxSUDRYH01Sg"
   },
   "source": [
    "<a name='2-2'></a>\n",
    "### <font color='darkviolet'> 2.2 - Sampling\n",
    "\n",
    "Assume the model has already been trained on a dataset of dinosaur names, and now you would like to generate new text, character by character. Here's how the generation process works:\n",
    "    \n",
    "<img src=\"images/dinos3.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center><font color='purple'><b>Figure 3</b>: You feed the initial input $x^{\\langle 1\\rangle} = \\vec{0}$ into the model at the first time-step, and the network samples one character at a time. </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2l1SKw3301Sg"
   },
   "source": [
    "<a name='ex-2'></a>\n",
    "### <font color='blue'> Exercise 2 \n",
    "\n",
    "Implement the `sample` function to sample characters. Follow these 4 steps:\n",
    "\n",
    "- **Step 1**: Initialize with a \"dummy\" vector of zeros $x^{\\langle 1 \\rangle} = \\vec{0}$. \n",
    "    - This is the default input before any characters are generated.\n",
    "    - Set $a^{\\langle 0 \\rangle} = \\vec{0}$ as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2VAXHjEd01Sh"
   },
   "source": [
    "- **Step 2**: Perform one step of forward propagation to compute $a^{\\langle 1 \\rangle}$ and $\\hat{y}^{\\langle 1 \\rangle}$. Here are the equations:\n",
    "\n",
    "\n",
    "*Hidden state* : \n",
    "$$ a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t+1 \\rangle } + W_{aa} a^{\\langle t \\rangle } + b)\\tag{1}$$\n",
    "\n",
    "*Prediction:*\n",
    "$$ \\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle }) = softmax(W_{ya}  a^{\\langle t + 1 \\rangle } + b_y) \\tag{2}$$\n",
    "\n",
    "Details about $\\hat{y}^{\\langle t+1 \\rangle }$:\n",
    "   - $\\hat{y}^{\\langle t+1 \\rangle }$ is a softmax probability vector, meaning its entries are between 0 and 1, and they sum to 1. \n",
    "   - $\\hat{y}^{\\langle t+1 \\rangle}_i$ represents the probability that the character indexed by \"i\" is the next character.  \n",
    "   - A `softmax()` function is provided for you to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4QkJOHAS01Sh"
   },
   "source": [
    "*Additional Hints*\n",
    "\n",
    "- $x^{\\langle 1 \\rangle}$ is `x` in the code. When creating the one-hot vector, make a numpy array of zeros, with the number of rows equal to the number of unique characters, and the number of columns equal to one. It's a 2D array and not a 1D array.\n",
    "- $a^{\\langle 0 \\rangle}$ is `a_prev` in the code.  It is a numpy array of zeros, where the number of rows is $n_{a}$, and number of columns is 1. $n_{a}$ is retrieved by getting the number of columns in $W_{aa}$ (the numbers need to match in order for the matrix multiplication $W_{aa}a^{\\langle t \\rangle}$ to work.\n",
    "- Official documentation for [numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) and [numpy.tanh](https://docs.scipy.org/doc/numpy/reference/generated/numpy.tanh.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8_GeJwei01Ss"
   },
   "source": [
    "- **Step 3**: Sampling. \n",
    "    - Now that you have $y^{\\langle t+1 \\rangle}$, you want to select the next letter in the dinosaur name. If you select the most probable, the model will always generate the same result given a starting letter. Use [np.random.choice](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.choice.html) to select a next letter that is *likely*, but not always the same.\n",
    "    - Pick the next character's **index** according to the probability distribution specified by $\\hat{y}^{\\langle t+1 \\rangle }$. \n",
    "    - This means that if $\\hat{y}^{\\langle t+1 \\rangle }_i = 0.16$, you will pick the index \"i\" with 16% probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KHJ4tfWY01Su"
   },
   "source": [
    "- **Step 4**: Updating $x^{\\langle t \\rangle }$ .\n",
    "    - The last step in `sample()` is to update the variable `x`, which currently stores $x^{\\langle t \\rangle }$, with the value of $x^{\\langle t + 1 \\rangle }$. \n",
    "    - To represent $x^{\\langle t + 1 \\rangle }$, create a one-hot vector corresponding to the character chosen as the prediction. \n",
    "    - Forward propagate $x^{\\langle t + 1 \\rangle }$ in Step 1 and repeate the process until a `\"\\n\"` character is encountered, indicating the end of the dinosaur name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UIkYdtBx01Su"
   },
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- Python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n",
    "    char_to_ix -- Python dictionary mapping each character to an index.\n",
    "\n",
    "    Returns:\n",
    "    indices -- A list of length n containing the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    # Step 1: Initialize input vector and hidden state\n",
    "    x = None # One-hot vector for the first character\n",
    "    a_prev = None # Initial hidden state\n",
    "    \n",
    "    # Initialize list to store sampled indices\n",
    "    indices = []\n",
    "    \n",
    "    # Initialize index\n",
    "    idx = -1 \n",
    "    \n",
    "    # Define the newline character index\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    counter = 0\n",
    "    \n",
    "    # Sampling loop\n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        # Step 2: Forward propagation to compute probabilities\n",
    "        a = None\n",
    "        z = None\n",
    "        y = None # 2D array containing the probability distribution for each character \n",
    "        \n",
    "        # Step 3: Sample a character index from the probability distribution\n",
    "        idx = None\n",
    "\n",
    "        # Append the index to the list of sampled indices\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # Step 4: Update the input vector for the next iteration\n",
    "        x = None\n",
    "        x[idx] = None\n",
    "        \n",
    "        # Update the previous hidden state\n",
    "        a_prev = a\n",
    "        \n",
    "        # Update counter\n",
    "        counter += 1\n",
    "\n",
    "    # Add newline character index if the loop was stopped\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_test(target):\n",
    "    np.random.seed(24)\n",
    "    _, n_a = 20, 100\n",
    "    Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "    b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "    indices = target(parameters, char_to_ix)\n",
    "    print(\"Sampling:\")\n",
    "    print(\"list of sampled indices:\\n\", indices)\n",
    "    print(\"list of sampled characters:\\n\", [ix_to_char[i] for i in indices])\n",
    "    \n",
    "    assert len(indices) < 52, \"Indices length must be smaller than 52\"\n",
    "    assert indices[-1] == char_to_ix['\\n'], \"All samples must end with \\\\n\"\n",
    "    assert min(indices) >= 0 and max(indices) < len(char_to_ix), f\"Sampled indexes must be between 0 and len(char_to_ix)={len(char_to_ix)}\"\n",
    "    assert np.allclose(indices[0:6], [23, 16, 26, 26, 24, 3]), \"Wrong values\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "\n",
    "sample_test(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "Sampling:\n",
    "list of sampled indices:\n",
    " [23, 16, 26, 26, 24, 3, 21, 1, 7, 24, 15, 3, 25, 20, 6, 13, 10, 8, 20, 12, 2, 0]\n",
    "list of sampled characters:\n",
    " ['w', 'p', 'z', 'z', 'x', 'c', 'u', 'a', 'g', 'x', 'o', 'c', 'y', 't', 'f', 'm', 'j', 'h', 't', 'l', 'b', '\\n']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qdeVJ9xT01S2"
   },
   "source": [
    "<a name='3'></a>\n",
    "## <font color='darkviolet'> 3 - The Language Model \n",
    "\n",
    "It's time to build the character-level language model for text generation! \n",
    "\n",
    "\n",
    "<a name='3-1'></a>\n",
    "### <font color='darkviolet'> 3.1 - Gradient Descent \n",
    "\n",
    "In this section you will implement a function performing one step of stochastic gradient descent (with clipped gradients). You'll go through the training examples one at a time, so the optimization algorithm will be stochastic gradient descent. \n",
    "\n",
    "As a reminder, here are the steps of a common optimization loop for an RNN:\n",
    "\n",
    "- Forward propagate through the RNN to compute the loss\n",
    "- Backward propagate through time to compute the gradients of the loss with respect to the parameters\n",
    "- Clip the gradients\n",
    "- Update the parameters using gradient descent \n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### <font color='blue'> Exercise 3\n",
    "\n",
    "Complete the implementation of the optimization process (one step of stochastic gradient descent).\n",
    "\n",
    "Provided functions:\n",
    "\n",
    "```python\n",
    "def rnn_forward(X, Y, a_prev, parameters):\n",
    "    \"\"\" \n",
    "    Forward propagation through the RNN to compute the cross-entropy loss.\n",
    "    \n",
    "    Args:\n",
    "    X -- Input data, list of integers representing characters' indices.\n",
    "    Y -- Target data, list of integers representing characters' indices shifted one index.\n",
    "    a_prev -- Previous hidden state.\n",
    "    parameters -- Model parameters (Wax, Waa, Wya, b, by).\n",
    "    \n",
    "    Returns:\n",
    "    loss -- Cross-entropy loss.\n",
    "    cache -- Cached values for backpropagation.\n",
    "    \"\"\"\n",
    "    # Implementation omitted\n",
    "    return loss, cache\n",
    "    \n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    \"\"\" Performs the backward propagation through time to compute the gradients of the loss with respect\n",
    "    to the parameters. It returns also all the hidden states.\"\"\"\n",
    "    # Implementation omitted\n",
    "    return gradients, a\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\" Updates parameters using the Gradient Descent Update Rule.\"\"\"\n",
    "    # Implementation omitted\n",
    "    return parameters\n",
    "```\n",
    "\n",
    "Recall that you previously implemented the `clip` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YsaPkvUq01S3"
   },
   "source": [
    "**Parameters**\n",
    "\n",
    "* Note that the weights and biases inside the `parameters` dictionary are being updated by the optimization, even though `parameters` is not one of the returned values of the `optimize` function. The `parameters` dictionary is passed by reference into the function, so changes to this dictionary are making changes to the `parameters` dictionary even when accessed outside of the function.\n",
    "* Python dictionaries and lists are \"pass by reference\". This means that if you pass a dictionary (or a list) into a function and modify it inside the function, you're actually modifying the original dictionary (or list), not a copy of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_BbEdIgY01S3"
   },
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- list of integers, each representing a character's index in the vocabulary.\n",
    "    Y -- list of integers, same as X but shifted one index to the left.\n",
    "    a_prev -- previous hidden state.\n",
    "    parameters -- dictionary containing:\n",
    "                    Wax -- Weight matrix for input-to-hidden, shape (n_a, n_x)\n",
    "                    Waa -- Weight matrix for hidden-to-hidden, shape (n_a, n_a)\n",
    "                    Wya -- Weight matrix for hidden-to-output, shape (n_y, n_a)\n",
    "                    b -- Bias vector for hidden layer, shape (n_a, 1)\n",
    "                    by -- Bias vector for output layer, shape (n_y, 1)\n",
    "    learning_rate -- learning rate for the model.\n",
    "    \n",
    "    Returns:\n",
    "    loss -- value of the loss function (cross-entropy)\n",
    "    gradients -- dictionary containing gradients:\n",
    "                    dWax -- Gradient of input-to-hidden weights, shape (n_a, n_x)\n",
    "                    dWaa -- Gradient of hidden-to-hidden weights, shape (n_a, n_a)\n",
    "                    dWya -- Gradient of hidden-to-output weights, shape (n_y, n_a)\n",
    "                    db -- Gradient of hidden layer bias, shape (n_a, 1)\n",
    "                    dby -- Gradient of output layer bias, shape (n_y, 1)\n",
    "    a[len(X)-1] -- last hidden state, shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Forward propagate through the RNN and compute the loss\n",
    "    loss, cache = None\n",
    "    \n",
    "    # Backpropagate through the RNN to compute gradients\n",
    "    gradients, a = None\n",
    "    \n",
    "    # Clip gradients between -5 (min) and 5 (max)\n",
    "    gradients = None\n",
    "    \n",
    "    # Update parameters using the gradients and learning rate\n",
    "    parameters = None\n",
    "    \n",
    "    return loss, gradients, a[len(X) - 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qZ63mN5-01S9"
   },
   "source": [
    "<a name='3-2'></a>\n",
    "## <font color='darkviolet'> 3.2 - Training the Model \n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### <font color='blue'>  Exercise 4 \n",
    "\n",
    "Complete the implementation of `model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data_x, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):\n",
    "   \n",
    "    \"\"\"\n",
    "    Trains the model and generates dinosaur names. \n",
    "\n",
    "    Arguments:\n",
    "    data_x -- Text corpus, divided into words.\n",
    "    ix_to_char -- Dictionary that maps the index to a character.\n",
    "    char_to_ix -- Dictionary that maps a character to an index.\n",
    "    num_iterations -- Number of iterations to train the model for.\n",
    "    n_a -- Number of units of the RNN cell.\n",
    "    dino_names -- Number of dinosaur names to sample at each iteration. \n",
    "    vocab_size -- Number of unique characters found in the text (size of the vocabulary).\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- Learned parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    # Initialize loss (this is required because we want to smooth our loss)\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "        \n",
    "    # Build list of all dinosaur names (training examples).\n",
    "    examples = [x.strip() for x in data_x]\n",
    "    \n",
    "    # Shuffle list of all dinosaur names\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    # Initialize the hidden state \n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "                \n",
    "        # Set the index `idx`into the list of examples\n",
    "        idx = None\n",
    "        \n",
    "        # Extract a single example from the list of examples    \n",
    "        X = None \n",
    "        \n",
    "        # Get the integer representation of the newline character '\\n'\n",
    "        ix_newline = None\n",
    "        \n",
    "        # Set the list of labels (integer representation of the characters)\n",
    "        Y = None\n",
    "\n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        # Choose a learning rate of 0.01\n",
    "        curr_loss, gradients, a_prev = None\n",
    "                   \n",
    "        # to keep the loss smooth.\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        # Every 2000 Iteration, generate \"n\" characters with sample() to check if the model is learning properly\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            seed = 0\n",
    "            # The number of dinosaur names to print\n",
    "            for name in range(dino_names):\n",
    "                \n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, char_to_ix)\n",
    "                last_dino_name = get_sample(sampled_indices, ix_to_char)\n",
    "                print(last_dino_name.replace('\\n', ''))\n",
    "                seed += 1\n",
    "                  \n",
    "            print('\\n')\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-HvBlAnS01TB"
   },
   "source": [
    "When you run the following cell, you should observe your model outputting random-looking characters at the first iteration. After a few thousand iterations, your model should learn to generate reasonable-looking names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3EH8Edc001TC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = model(data.split(\"\\n\"), ix_to_char, char_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of expected output**\n",
    "```\n",
    "...\n",
    "Iteration: 22000, Loss: 22.728886\n",
    "\n",
    "Onustreofkelus\n",
    "Llecagosaurus\n",
    "Mystolojmiaterltasaurus\n",
    "Ola\n",
    "Yuskeolongus\n",
    "Eiacosaurus\n",
    "Trodonosaurus\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Feel free to run the algorithm even longer and play with hyperparameters to see if you can get even better results! Your model hopefully learned that dinosaur names tend to end in saurus, don, aura, tor, etc.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Dinosaurus_Island_Character_level_language_model_final_v3b+Proposed.ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "DLSC5W1-A2"
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
